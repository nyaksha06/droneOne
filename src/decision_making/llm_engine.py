import asyncio
import logging
import sys
import requests
import json

logger = logging.getLogger(__name__)

class LLMDecisionEngine:
    """
    Communicates with the Ollama LLM to get drone action recommendations
    based on the current drone state and mission objectives.
    """

    def __init__(self, ollama_api_url: str, ollama_model_name: str):
        self.ollama_api_url = ollama_api_url
        self.ollama_model_name = ollama_model_name
        self.headers = {"Content-Type": "application/json"}
        logger.info(f"LLMDecisionEngine initialized for model '{ollama_model_name}' at {ollama_api_url}")

    async def get_action_from_llm(self, prompt_text: str) -> dict:
        """
        Sends the prompt to the Ollama LLM and parses the response into a structured action.
        This method is asynchronous to fit into our event loop, even if requests.post is blocking.
        (For true async HTTP, you'd use aiohttp or httpx with asyncio.)

        :param prompt_text: The detailed context prompt generated by DroneState.
        :return: A dictionary representing the recommended drone action, or an empty dict on failure.
                 Example: {"action": "goto_location", "latitude": X, "longitude": Y, "altitude": Z}
                          {"action": "land"}
                          {"action": "do_nothing"}
        """
        logger.info("Requesting action from LLM...")
        payload = {
            "model": self.ollama_model_name,
            "prompt": prompt_text,
            "stream": False # We want a single response
            # "options": {"temperature": 0.5, "num_predict": 128} # Example options
        }

        try:
            # Using loop.run_in_executor to make the blocking requests.post call non-blocking
            # for the asyncio event loop.
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None, # Use default thread pool executor
                lambda: requests.post(self.ollama_api_url, headers=self.headers, data=json.dumps(payload), timeout=30)
            )
            
            response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)
            result = response.json()

            # Ollama's /api/generate usually returns a 'response' field with the generated text
            llm_raw_response_text = result.get("response", "").strip()
            logger.info(f"LLM Raw Response: {llm_raw_response_text}")

            # --- Simple Parsing Logic (will need refinement) ---
            # This is a very basic example. In a real system, you'd use more robust parsing
            # or ask the LLM to output strict JSON.
            action = self._parse_llm_response(llm_raw_response_text)
            
            if not action:
                logger.warning("LLM response could not be parsed into a valid action. Defaulting to 'do_nothing'.")
                return {"action": "do_nothing", "reason": "parsing_failed"}

            logger.info(f"LLM Recommended Action: {action}")
            return action

        except requests.exceptions.RequestException as e:
            logger.error(f"Error communicating with Ollama API: {e}")
            return {"action": "do_nothing", "reason": f"ollama_api_error: {e}"}
        except json.JSONDecodeError as e:
            logger.error(f"Error decoding LLM response JSON: {e}. Response text: {response.text}")
            return {"action": "do_nothing", "reason": f"json_decode_error: {e}"}
        except Exception as e:
            logger.error(f"An unexpected error occurred in get_action_from_llm: {e}")
            return {"action": "do_nothing", "reason": f"unexpected_error: {e}"}

    def _parse_llm_response(self, response_text: str) -> dict:
        """
        Parses the raw text response from the LLM into a structured dictionary.
        This is a placeholder and will need significant development based on
        how you train/prompt your LLM to format its output.
        
        For initial testing, let's assume the LLM provides simple commands.
        """
        response_text = response_text.lower().strip()

        if "takeoff" in response_text and "altitude" in response_text:
            try:
                # Example: "takeoff to altitude 10 meters"
                parts = response_text.split("altitude")
                alt_str = parts[1].split("meters")[0].strip()
                altitude = float(alt_str)
                return {"action": "takeoff", "altitude_m": altitude}
            except (IndexError, ValueError):
                return {"action": "takeoff", "altitude_m": 2.5} # Default if parse fails
        elif "land" in response_text:
            return {"action": "land"}
        elif "go to" in response_text or "fly to" in response_text:
            # Example: "go to lat 47.3976, lon 8.5456, alt 20"
            try:
                lat_match = [part for part in response_text.split("lat") if part]
                lon_match = [part for part in response_text.split("lon") if part]
                alt_match = [part for part in response_text.split("alt") if part]
                
                latitude = float(lat_match[0].split(',')[0].strip()) if lat_match else 0.0
                longitude = float(lon_match[0].split(',')[0].strip()) if lon_match else 0.0
                altitude = float(alt_match[0].split(',')[0].strip()) if alt_match else 10.0
                
                return {"action": "goto_location", "latitude_deg": latitude, "longitude_deg": longitude, "altitude_m": altitude}
            except (IndexError, ValueError) as e:
                logger.warning(f"Failed to parse goto coordinates: {e}. Response: {response_text}")
                return {"action": "do_nothing", "reason": "goto_parse_error"}
        elif "do nothing" in response_text or "wait" in response_text:
            return {"action": "do_nothing"}
        else:
            return {} # Unrecognized command

